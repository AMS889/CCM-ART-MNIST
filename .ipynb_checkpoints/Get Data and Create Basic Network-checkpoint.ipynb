{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import helper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io as spio\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "rs = 2018\n",
    "random.seed(rs)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the matrix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24000, 24000, 24000, 24000, 24000, 24000, 24000, 24000, 24000,\n",
       "       24000], dtype=uint64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist = spio.loadmat(\"data/EMNIST/emnist-digits.mat\")\n",
    "x_train = emnist[\"dataset\"][0][0][0][0][0][0].astype(np.float32)\n",
    "y_train = emnist[\"dataset\"][0][0][0][0][0][1]\n",
    "\n",
    "x_test = emnist[\"dataset\"][0][0][1][0][0][0].astype(np.float32)\n",
    "y_test = emnist[\"dataset\"][0][0][1][0][0][1]\n",
    "\n",
    "x_train = x_train/x_train.max()\n",
    "x_test = x_test/x_train.max()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28, order = 'A').astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28, order = 'A').astype('float32')\n",
    "\n",
    "y_train = pd.get_dummies(y_train.ravel()).values\n",
    "y_test = pd.get_dummies(y_test.ravel()).values\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "y_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Tensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(rs)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST('data/', train=True, split='digits', download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST('data/', train=False, split='digits', transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # first convolutional layer\n",
    "        h_conv1 = self.conv1(x)\n",
    "        h_conv1 = F.relu(h_conv1)\n",
    "        h_conv1_pool = F.max_pool2d(h_conv1, 2)\n",
    "\n",
    "        # second convolutional layer\n",
    "        h_conv2 = self.conv2(h_conv1_pool)\n",
    "        h_conv2 = F.relu(h_conv2)\n",
    "        h_conv2_pool = F.max_pool2d(h_conv2, 2)\n",
    "\n",
    "        # fully-connected layer\n",
    "        h_fc1 = h_conv2_pool.view(-1, 320)\n",
    "        h_fc1 = self.fc1(h_fc1)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "        \n",
    "        # classifier output\n",
    "        output = self.fc2(h_fc1)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, h_fc1, h_conv2, h_conv1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rhammond/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/240000 (0%)]\tLoss: 2.311247\n",
      "Train Epoch: 0 [10000/240000 (4%)]\tLoss: 2.300393\n",
      "Train Epoch: 0 [20000/240000 (8%)]\tLoss: 2.294405\n",
      "Train Epoch: 0 [30000/240000 (12%)]\tLoss: 2.284554\n",
      "Train Epoch: 0 [40000/240000 (17%)]\tLoss: 2.266620\n",
      "Train Epoch: 0 [50000/240000 (21%)]\tLoss: 2.250103\n",
      "Train Epoch: 0 [60000/240000 (25%)]\tLoss: 2.216611\n",
      "Train Epoch: 0 [70000/240000 (29%)]\tLoss: 2.173913\n",
      "Train Epoch: 0 [80000/240000 (33%)]\tLoss: 2.111950\n",
      "Train Epoch: 0 [90000/240000 (38%)]\tLoss: 1.996463\n",
      "Train Epoch: 0 [100000/240000 (42%)]\tLoss: 1.782908\n",
      "Train Epoch: 0 [110000/240000 (46%)]\tLoss: 1.477559\n",
      "Train Epoch: 0 [120000/240000 (50%)]\tLoss: 1.132487\n",
      "Train Epoch: 0 [130000/240000 (54%)]\tLoss: 0.830862\n",
      "Train Epoch: 0 [140000/240000 (58%)]\tLoss: 0.675104\n",
      "Train Epoch: 0 [150000/240000 (62%)]\tLoss: 0.548510\n",
      "Train Epoch: 0 [160000/240000 (67%)]\tLoss: 0.519072\n",
      "Train Epoch: 0 [170000/240000 (71%)]\tLoss: 0.440623\n",
      "Train Epoch: 0 [180000/240000 (75%)]\tLoss: 0.411442\n",
      "Train Epoch: 0 [190000/240000 (79%)]\tLoss: 0.382288\n",
      "Train Epoch: 0 [200000/240000 (83%)]\tLoss: 0.354773\n",
      "Train Epoch: 0 [210000/240000 (88%)]\tLoss: 0.317286\n",
      "Train Epoch: 0 [220000/240000 (92%)]\tLoss: 0.313069\n",
      "Train Epoch: 0 [230000/240000 (96%)]\tLoss: 0.306206\n",
      "Time elapsed 0:01:34.415189\n",
      "\n",
      "Test set: Average loss: 0.2992, Accuracy: 36434/40000 (91%)\n",
      "\n",
      "Train Epoch: 1 [0/240000 (0%)]\tLoss: 0.284674\n",
      "Train Epoch: 1 [10000/240000 (4%)]\tLoss: 0.274618\n",
      "Train Epoch: 1 [20000/240000 (8%)]\tLoss: 0.300535\n",
      "Train Epoch: 1 [30000/240000 (12%)]\tLoss: 0.269056\n",
      "Train Epoch: 1 [40000/240000 (17%)]\tLoss: 0.254437\n",
      "Train Epoch: 1 [50000/240000 (21%)]\tLoss: 0.219227\n",
      "Train Epoch: 1 [60000/240000 (25%)]\tLoss: 0.255432\n",
      "Train Epoch: 1 [70000/240000 (29%)]\tLoss: 0.254315\n",
      "Train Epoch: 1 [80000/240000 (33%)]\tLoss: 0.240314\n",
      "Train Epoch: 1 [90000/240000 (38%)]\tLoss: 0.231928\n",
      "Train Epoch: 1 [100000/240000 (42%)]\tLoss: 0.257667\n",
      "Train Epoch: 1 [110000/240000 (46%)]\tLoss: 0.254418\n",
      "Train Epoch: 1 [120000/240000 (50%)]\tLoss: 0.221844\n",
      "Train Epoch: 1 [130000/240000 (54%)]\tLoss: 0.236400\n",
      "Train Epoch: 1 [140000/240000 (58%)]\tLoss: 0.218622\n",
      "Train Epoch: 1 [150000/240000 (62%)]\tLoss: 0.196148\n",
      "Train Epoch: 1 [160000/240000 (67%)]\tLoss: 0.254564\n",
      "Train Epoch: 1 [170000/240000 (71%)]\tLoss: 0.189566\n",
      "Train Epoch: 1 [180000/240000 (75%)]\tLoss: 0.191779\n",
      "Train Epoch: 1 [190000/240000 (79%)]\tLoss: 0.204913\n",
      "Train Epoch: 1 [200000/240000 (83%)]\tLoss: 0.159114\n",
      "Train Epoch: 1 [210000/240000 (88%)]\tLoss: 0.166379\n",
      "Train Epoch: 1 [220000/240000 (92%)]\tLoss: 0.217023\n",
      "Train Epoch: 1 [230000/240000 (96%)]\tLoss: 0.170996\n",
      "Time elapsed 0:03:14.318613\n",
      "\n",
      "Test set: Average loss: 0.1743, Accuracy: 37933/40000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/240000 (0%)]\tLoss: 0.207922\n",
      "Train Epoch: 2 [10000/240000 (4%)]\tLoss: 0.185311\n",
      "Train Epoch: 2 [20000/240000 (8%)]\tLoss: 0.172182\n",
      "Train Epoch: 2 [30000/240000 (12%)]\tLoss: 0.131414\n",
      "Train Epoch: 2 [40000/240000 (17%)]\tLoss: 0.175120\n",
      "Train Epoch: 2 [50000/240000 (21%)]\tLoss: 0.181397\n",
      "Train Epoch: 2 [60000/240000 (25%)]\tLoss: 0.147032\n",
      "Train Epoch: 2 [70000/240000 (29%)]\tLoss: 0.142171\n",
      "Train Epoch: 2 [80000/240000 (33%)]\tLoss: 0.130951\n",
      "Train Epoch: 2 [90000/240000 (38%)]\tLoss: 0.104435\n",
      "Train Epoch: 2 [100000/240000 (42%)]\tLoss: 0.161099\n",
      "Train Epoch: 2 [110000/240000 (46%)]\tLoss: 0.162156\n",
      "Train Epoch: 2 [120000/240000 (50%)]\tLoss: 0.178453\n",
      "Train Epoch: 2 [130000/240000 (54%)]\tLoss: 0.185619\n",
      "Train Epoch: 2 [140000/240000 (58%)]\tLoss: 0.154785\n",
      "Train Epoch: 2 [150000/240000 (62%)]\tLoss: 0.155091\n",
      "Train Epoch: 2 [160000/240000 (67%)]\tLoss: 0.158529\n",
      "Train Epoch: 2 [170000/240000 (71%)]\tLoss: 0.133137\n",
      "Train Epoch: 2 [180000/240000 (75%)]\tLoss: 0.105564\n",
      "Train Epoch: 2 [190000/240000 (79%)]\tLoss: 0.119870\n",
      "Train Epoch: 2 [200000/240000 (83%)]\tLoss: 0.150192\n",
      "Train Epoch: 2 [210000/240000 (88%)]\tLoss: 0.138843\n",
      "Train Epoch: 2 [220000/240000 (92%)]\tLoss: 0.140888\n",
      "Train Epoch: 2 [230000/240000 (96%)]\tLoss: 0.113747\n",
      "Time elapsed 0:04:54.240698\n",
      "\n",
      "Test set: Average loss: 0.1259, Accuracy: 38526/40000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/240000 (0%)]\tLoss: 0.112750\n",
      "Train Epoch: 3 [10000/240000 (4%)]\tLoss: 0.131177\n",
      "Train Epoch: 3 [20000/240000 (8%)]\tLoss: 0.136213\n",
      "Train Epoch: 3 [30000/240000 (12%)]\tLoss: 0.150761\n",
      "Train Epoch: 3 [40000/240000 (17%)]\tLoss: 0.121663\n",
      "Train Epoch: 3 [50000/240000 (21%)]\tLoss: 0.126183\n",
      "Train Epoch: 3 [60000/240000 (25%)]\tLoss: 0.116894\n",
      "Train Epoch: 3 [70000/240000 (29%)]\tLoss: 0.128020\n",
      "Train Epoch: 3 [80000/240000 (33%)]\tLoss: 0.113541\n",
      "Train Epoch: 3 [90000/240000 (38%)]\tLoss: 0.125677\n",
      "Train Epoch: 3 [100000/240000 (42%)]\tLoss: 0.096199\n",
      "Train Epoch: 3 [110000/240000 (46%)]\tLoss: 0.119082\n",
      "Train Epoch: 3 [120000/240000 (50%)]\tLoss: 0.094843\n",
      "Train Epoch: 3 [130000/240000 (54%)]\tLoss: 0.104135\n",
      "Train Epoch: 3 [140000/240000 (58%)]\tLoss: 0.138067\n",
      "Train Epoch: 3 [150000/240000 (62%)]\tLoss: 0.087766\n",
      "Train Epoch: 3 [160000/240000 (67%)]\tLoss: 0.107640\n",
      "Train Epoch: 3 [170000/240000 (71%)]\tLoss: 0.088407\n",
      "Train Epoch: 3 [180000/240000 (75%)]\tLoss: 0.110967\n",
      "Train Epoch: 3 [190000/240000 (79%)]\tLoss: 0.084422\n",
      "Train Epoch: 3 [200000/240000 (83%)]\tLoss: 0.107046\n",
      "Train Epoch: 3 [210000/240000 (88%)]\tLoss: 0.091182\n",
      "Train Epoch: 3 [220000/240000 (92%)]\tLoss: 0.110833\n",
      "Train Epoch: 3 [230000/240000 (96%)]\tLoss: 0.083566\n",
      "Time elapsed 0:06:34.320250\n",
      "\n",
      "Test set: Average loss: 0.1014, Accuracy: 38791/40000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/240000 (0%)]\tLoss: 0.133052\n",
      "Train Epoch: 4 [10000/240000 (4%)]\tLoss: 0.085488\n",
      "Train Epoch: 4 [20000/240000 (8%)]\tLoss: 0.107717\n",
      "Train Epoch: 4 [30000/240000 (12%)]\tLoss: 0.093343\n",
      "Train Epoch: 4 [40000/240000 (17%)]\tLoss: 0.093867\n",
      "Train Epoch: 4 [50000/240000 (21%)]\tLoss: 0.103951\n",
      "Train Epoch: 4 [60000/240000 (25%)]\tLoss: 0.104297\n",
      "Train Epoch: 4 [70000/240000 (29%)]\tLoss: 0.092856\n",
      "Train Epoch: 4 [80000/240000 (33%)]\tLoss: 0.080340\n",
      "Train Epoch: 4 [90000/240000 (38%)]\tLoss: 0.080077\n",
      "Train Epoch: 4 [100000/240000 (42%)]\tLoss: 0.088825\n",
      "Train Epoch: 4 [110000/240000 (46%)]\tLoss: 0.090813\n",
      "Train Epoch: 4 [120000/240000 (50%)]\tLoss: 0.118927\n",
      "Train Epoch: 4 [130000/240000 (54%)]\tLoss: 0.095134\n",
      "Train Epoch: 4 [140000/240000 (58%)]\tLoss: 0.104823\n",
      "Train Epoch: 4 [150000/240000 (62%)]\tLoss: 0.106792\n",
      "Train Epoch: 4 [160000/240000 (67%)]\tLoss: 0.112784\n",
      "Train Epoch: 4 [170000/240000 (71%)]\tLoss: 0.119842\n",
      "Train Epoch: 4 [180000/240000 (75%)]\tLoss: 0.079989\n",
      "Train Epoch: 4 [190000/240000 (79%)]\tLoss: 0.098981\n",
      "Train Epoch: 4 [200000/240000 (83%)]\tLoss: 0.068834\n",
      "Train Epoch: 4 [210000/240000 (88%)]\tLoss: 0.091745\n",
      "Train Epoch: 4 [220000/240000 (92%)]\tLoss: 0.081233\n",
      "Train Epoch: 4 [230000/240000 (96%)]\tLoss: 0.108165\n",
      "Time elapsed 0:08:14.023959\n",
      "\n",
      "Test set: Average loss: 0.0887, Accuracy: 38977/40000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/240000 (0%)]\tLoss: 0.093129\n",
      "Train Epoch: 5 [10000/240000 (4%)]\tLoss: 0.100339\n",
      "Train Epoch: 5 [20000/240000 (8%)]\tLoss: 0.107040\n",
      "Train Epoch: 5 [30000/240000 (12%)]\tLoss: 0.078163\n",
      "Train Epoch: 5 [40000/240000 (17%)]\tLoss: 0.075842\n",
      "Train Epoch: 5 [50000/240000 (21%)]\tLoss: 0.067418\n",
      "Train Epoch: 5 [60000/240000 (25%)]\tLoss: 0.084615\n",
      "Train Epoch: 5 [70000/240000 (29%)]\tLoss: 0.114622\n",
      "Train Epoch: 5 [80000/240000 (33%)]\tLoss: 0.091198\n",
      "Train Epoch: 5 [90000/240000 (38%)]\tLoss: 0.089817\n",
      "Train Epoch: 5 [100000/240000 (42%)]\tLoss: 0.085590\n",
      "Train Epoch: 5 [110000/240000 (46%)]\tLoss: 0.099349\n",
      "Train Epoch: 5 [120000/240000 (50%)]\tLoss: 0.094690\n",
      "Train Epoch: 5 [130000/240000 (54%)]\tLoss: 0.086757\n",
      "Train Epoch: 5 [140000/240000 (58%)]\tLoss: 0.081181\n",
      "Train Epoch: 5 [150000/240000 (62%)]\tLoss: 0.086024\n",
      "Train Epoch: 5 [160000/240000 (67%)]\tLoss: 0.079523\n",
      "Train Epoch: 5 [170000/240000 (71%)]\tLoss: 0.085651\n",
      "Train Epoch: 5 [180000/240000 (75%)]\tLoss: 0.093969\n",
      "Train Epoch: 5 [190000/240000 (79%)]\tLoss: 0.071424\n",
      "Train Epoch: 5 [200000/240000 (83%)]\tLoss: 0.094395\n",
      "Train Epoch: 5 [210000/240000 (88%)]\tLoss: 0.070303\n",
      "Train Epoch: 5 [220000/240000 (92%)]\tLoss: 0.075290\n",
      "Train Epoch: 5 [230000/240000 (96%)]\tLoss: 0.069070\n",
      "Time elapsed 0:09:53.621644\n",
      "\n",
      "Test set: Average loss: 0.0757, Accuracy: 39107/40000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/240000 (0%)]\tLoss: 0.078902\n",
      "Train Epoch: 6 [10000/240000 (4%)]\tLoss: 0.105188\n",
      "Train Epoch: 6 [20000/240000 (8%)]\tLoss: 0.071218\n",
      "Train Epoch: 6 [30000/240000 (12%)]\tLoss: 0.073416\n",
      "Train Epoch: 6 [40000/240000 (17%)]\tLoss: 0.063479\n",
      "Train Epoch: 6 [50000/240000 (21%)]\tLoss: 0.081543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [60000/240000 (25%)]\tLoss: 0.056964\n",
      "Train Epoch: 6 [70000/240000 (29%)]\tLoss: 0.074061\n",
      "Train Epoch: 6 [80000/240000 (33%)]\tLoss: 0.056149\n",
      "Train Epoch: 6 [90000/240000 (38%)]\tLoss: 0.062360\n",
      "Train Epoch: 6 [100000/240000 (42%)]\tLoss: 0.059369\n",
      "Train Epoch: 6 [110000/240000 (46%)]\tLoss: 0.061706\n",
      "Train Epoch: 6 [120000/240000 (50%)]\tLoss: 0.061936\n",
      "Train Epoch: 6 [130000/240000 (54%)]\tLoss: 0.084760\n",
      "Train Epoch: 6 [140000/240000 (58%)]\tLoss: 0.092338\n",
      "Train Epoch: 6 [150000/240000 (62%)]\tLoss: 0.073131\n",
      "Train Epoch: 6 [160000/240000 (67%)]\tLoss: 0.053136\n",
      "Train Epoch: 6 [170000/240000 (71%)]\tLoss: 0.067487\n",
      "Train Epoch: 6 [180000/240000 (75%)]\tLoss: 0.092997\n",
      "Train Epoch: 6 [190000/240000 (79%)]\tLoss: 0.080027\n",
      "Train Epoch: 6 [200000/240000 (83%)]\tLoss: 0.072584\n",
      "Train Epoch: 6 [210000/240000 (88%)]\tLoss: 0.076966\n",
      "Train Epoch: 6 [220000/240000 (92%)]\tLoss: 0.094350\n",
      "Train Epoch: 6 [230000/240000 (96%)]\tLoss: 0.070092\n",
      "Time elapsed 0:11:33.530390\n",
      "\n",
      "Test set: Average loss: 0.0710, Accuracy: 39142/40000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/240000 (0%)]\tLoss: 0.083388\n",
      "Train Epoch: 7 [10000/240000 (4%)]\tLoss: 0.063901\n",
      "Train Epoch: 7 [20000/240000 (8%)]\tLoss: 0.043358\n",
      "Train Epoch: 7 [30000/240000 (12%)]\tLoss: 0.081917\n",
      "Train Epoch: 7 [40000/240000 (17%)]\tLoss: 0.068387\n",
      "Train Epoch: 7 [50000/240000 (21%)]\tLoss: 0.060170\n",
      "Train Epoch: 7 [60000/240000 (25%)]\tLoss: 0.064990\n",
      "Train Epoch: 7 [70000/240000 (29%)]\tLoss: 0.077553\n",
      "Train Epoch: 7 [80000/240000 (33%)]\tLoss: 0.079187\n",
      "Train Epoch: 7 [90000/240000 (38%)]\tLoss: 0.041349\n",
      "Train Epoch: 7 [100000/240000 (42%)]\tLoss: 0.062941\n",
      "Train Epoch: 7 [110000/240000 (46%)]\tLoss: 0.066998\n",
      "Train Epoch: 7 [120000/240000 (50%)]\tLoss: 0.084231\n",
      "Train Epoch: 7 [130000/240000 (54%)]\tLoss: 0.083850\n",
      "Train Epoch: 7 [140000/240000 (58%)]\tLoss: 0.082324\n",
      "Train Epoch: 7 [150000/240000 (62%)]\tLoss: 0.073537\n",
      "Train Epoch: 7 [160000/240000 (67%)]\tLoss: 0.088325\n",
      "Train Epoch: 7 [170000/240000 (71%)]\tLoss: 0.069209\n",
      "Train Epoch: 7 [180000/240000 (75%)]\tLoss: 0.083287\n",
      "Train Epoch: 7 [190000/240000 (79%)]\tLoss: 0.053896\n",
      "Train Epoch: 7 [200000/240000 (83%)]\tLoss: 0.066932\n",
      "Train Epoch: 7 [210000/240000 (88%)]\tLoss: 0.064058\n",
      "Train Epoch: 7 [220000/240000 (92%)]\tLoss: 0.050906\n",
      "Train Epoch: 7 [230000/240000 (96%)]\tLoss: 0.075563\n",
      "Time elapsed 0:13:13.643538\n",
      "\n",
      "Test set: Average loss: 0.0650, Accuracy: 39212/40000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/240000 (0%)]\tLoss: 0.065625\n",
      "Train Epoch: 8 [10000/240000 (4%)]\tLoss: 0.056297\n",
      "Train Epoch: 8 [20000/240000 (8%)]\tLoss: 0.085776\n",
      "Train Epoch: 8 [30000/240000 (12%)]\tLoss: 0.067932\n",
      "Train Epoch: 8 [40000/240000 (17%)]\tLoss: 0.053721\n",
      "Train Epoch: 8 [50000/240000 (21%)]\tLoss: 0.047452\n",
      "Train Epoch: 8 [60000/240000 (25%)]\tLoss: 0.087429\n",
      "Train Epoch: 8 [70000/240000 (29%)]\tLoss: 0.061413\n",
      "Train Epoch: 8 [80000/240000 (33%)]\tLoss: 0.074832\n",
      "Train Epoch: 8 [90000/240000 (38%)]\tLoss: 0.068653\n",
      "Train Epoch: 8 [100000/240000 (42%)]\tLoss: 0.049138\n",
      "Train Epoch: 8 [110000/240000 (46%)]\tLoss: 0.046192\n",
      "Train Epoch: 8 [120000/240000 (50%)]\tLoss: 0.056097\n",
      "Train Epoch: 8 [130000/240000 (54%)]\tLoss: 0.067158\n",
      "Train Epoch: 8 [140000/240000 (58%)]\tLoss: 0.062796\n",
      "Train Epoch: 8 [150000/240000 (62%)]\tLoss: 0.062736\n",
      "Train Epoch: 8 [160000/240000 (67%)]\tLoss: 0.063440\n",
      "Train Epoch: 8 [170000/240000 (71%)]\tLoss: 0.054691\n",
      "Train Epoch: 8 [180000/240000 (75%)]\tLoss: 0.041280\n",
      "Train Epoch: 8 [190000/240000 (79%)]\tLoss: 0.045683\n",
      "Train Epoch: 8 [200000/240000 (83%)]\tLoss: 0.084784\n",
      "Train Epoch: 8 [210000/240000 (88%)]\tLoss: 0.089778\n",
      "Train Epoch: 8 [220000/240000 (92%)]\tLoss: 0.063463\n",
      "Train Epoch: 8 [230000/240000 (96%)]\tLoss: 0.080695\n",
      "Time elapsed 0:14:56.822832\n",
      "\n",
      "Test set: Average loss: 0.0611, Accuracy: 39255/40000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/240000 (0%)]\tLoss: 0.049195\n",
      "Train Epoch: 9 [10000/240000 (4%)]\tLoss: 0.061208\n",
      "Train Epoch: 9 [20000/240000 (8%)]\tLoss: 0.060994\n",
      "Train Epoch: 9 [30000/240000 (12%)]\tLoss: 0.070227\n",
      "Train Epoch: 9 [40000/240000 (17%)]\tLoss: 0.066136\n",
      "Train Epoch: 9 [50000/240000 (21%)]\tLoss: 0.090984\n",
      "Train Epoch: 9 [60000/240000 (25%)]\tLoss: 0.055501\n",
      "Train Epoch: 9 [70000/240000 (29%)]\tLoss: 0.041508\n",
      "Train Epoch: 9 [80000/240000 (33%)]\tLoss: 0.067079\n",
      "Train Epoch: 9 [90000/240000 (38%)]\tLoss: 0.077437\n",
      "Train Epoch: 9 [100000/240000 (42%)]\tLoss: 0.055485\n",
      "Train Epoch: 9 [110000/240000 (46%)]\tLoss: 0.036998\n",
      "Train Epoch: 9 [120000/240000 (50%)]\tLoss: 0.058957\n",
      "Train Epoch: 9 [130000/240000 (54%)]\tLoss: 0.066537\n",
      "Train Epoch: 9 [140000/240000 (58%)]\tLoss: 0.054508\n",
      "Train Epoch: 9 [150000/240000 (62%)]\tLoss: 0.044111\n",
      "Train Epoch: 9 [160000/240000 (67%)]\tLoss: 0.057688\n",
      "Train Epoch: 9 [170000/240000 (71%)]\tLoss: 0.050667\n",
      "Train Epoch: 9 [180000/240000 (75%)]\tLoss: 0.072927\n",
      "Train Epoch: 9 [190000/240000 (79%)]\tLoss: 0.046633\n",
      "Train Epoch: 9 [200000/240000 (83%)]\tLoss: 0.080094\n",
      "Train Epoch: 9 [210000/240000 (88%)]\tLoss: 0.071176\n",
      "Train Epoch: 9 [220000/240000 (92%)]\tLoss: 0.047722\n",
      "Train Epoch: 9 [230000/240000 (96%)]\tLoss: 0.067488\n",
      "Time elapsed 0:16:57.995392\n",
      "\n",
      "Test set: Average loss: 0.0557, Accuracy: 39339/40000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "start = datetime.now() \n",
    "for epoch in range(10):\n",
    "    train(epoch)\n",
    "    print('Time elapsed {}'.format(datetime.now() - start))\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rhammond/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0557, Accuracy: 39339/40000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO STILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all():\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        # run the data through the network\n",
    "        output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "        \n",
    "        # compare prediction to ground truth\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().tolist()\n",
    "    perc_correct = 100. * correct / len(test_loader.dataset)\n",
    "    return perc_correct\n",
    "\n",
    "def test_viz(nshow=10):\n",
    "    \n",
    "    # grab a random subset of the data\n",
    "    testiter = iter(test_loader)\n",
    "    images, target = testiter.next()\n",
    "    perm = np.random.permutation(images.size()[0])\n",
    "    sel = torch.LongTensor(perm[:nshow])\n",
    "    images = images[sel]\n",
    "    data = Variable(images, volatile=True)\n",
    "    \n",
    "    # get predictions from the network\n",
    "    output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    pred = pred.numpy().flatten()\n",
    "    \n",
    "    # plot predictions along with the images\n",
    "    for i in range(nshow):\n",
    "        ax = plt.subplot(1, nshow, i+1)\n",
    "        imshow(utils.make_grid(images[i]))\n",
    "        plt.title(str(pred[i]))\n",
    "        \n",
    "def imshow(img):\n",
    "    img = 1 - (img * 0.3081 + 0.1307) # invert image pre-processing\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rhammond/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/rhammond/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.3475"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rhammond/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/Users/rhammond/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABLCAYAAABgOHyfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j9DXt47QRzZbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a193e07f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
