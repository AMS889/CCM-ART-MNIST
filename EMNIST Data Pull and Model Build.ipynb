{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import codecs\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import io as spio\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.callbacks import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2018) #setting a random seed so work is reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist = spio.loadmat(\"data/EMNIST/emnist-digits.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the training and test sets\n",
    "x_train = emnist[\"dataset\"][0][0][0][0][0][0].astype(np.float32)\n",
    "y_train = emnist[\"dataset\"][0][0][0][0][0][1]\n",
    "\n",
    "x_test = emnist[\"dataset\"][0][0][1][0][0][0].astype(np.float32)\n",
    "y_test = emnist[\"dataset\"][0][0][1][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing since 255 is the max value\n",
    "x_train = x_train/x_train.max()\n",
    "x_test = x_test/x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening pixels into vectors using order A\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28, order = 'A').astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28, order = 'A').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode the labels\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(20, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(20, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(20, (2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(20, (1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240000 samples, validate on 40000 samples\n",
      "Epoch 1/1\n",
      "240000/240000 [==============================] - 422s 2ms/step - loss: 0.5153 - acc: 0.8308 - val_loss: 0.8025 - val_acc: 0.9493\n",
      "Large CNN Error: 5.07%\n"
     ]
    }
   ],
   "source": [
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=1, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layers):\n",
    "    if layers == 1:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(20, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    if layers == 2:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(20, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(20, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    if layers == 3:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(20, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(20, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(20, (2, 2), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    if layers == 4:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(20, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(20, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(20, (2, 2), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(20, (1, 1), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(2)\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=200)\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240000 samples, validate on 40000 samples\n",
      "Epoch 1/1\n",
      "240000/240000 [==============================] - 798s 3ms/step - loss: 0.1210 - acc: 0.9649 - val_loss: 0.2757 - val_acc: 0.9827\n",
      "Large CNN Error: 1.73%\n",
      "Train on 240000 samples, validate on 40000 samples\n",
      "Epoch 1/2\n",
      "240000/240000 [==============================] - 846s 4ms/step - loss: 0.1261 - acc: 0.9633 - val_loss: 0.4679 - val_acc: 0.9705\n",
      "Epoch 2/2\n",
      "240000/240000 [==============================] - 907s 4ms/step - loss: 0.0415 - acc: 0.9875 - val_loss: 0.4000 - val_acc: 0.9749\n",
      "Large CNN Error: 2.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Parameter Tuning\n",
    "epochs= [5,10,20,25,30]\n",
    "batch_size= [50,100,150,200,250] #[50,100,150,200,250]\n",
    "layers=[1,2,3,4]  \n",
    "history=History()\n",
    "\n",
    "score=[]\n",
    "data_dict={}\n",
    "key= 0 \n",
    "for layer in layers:\n",
    "    for epoch in epochs:\n",
    "        for batch in batch_size:\n",
    "            model = create_model(layer)\n",
    "            model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epoch, batch_size=batch, callbacks=[history])\n",
    "            scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "            print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "            \n",
    "            #Save Iterations in Dictionary\n",
    "            data_dict[key]={'layer':layer, 'epoch':epoch, 'batch_size':batch}\n",
    "            score.append(scores[1])\n",
    "            key += 1  #update Dictionary key\n",
    "            \n",
    "            # summarize history for accuracy\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.savefig('results/plots/acc'+ '%s-%s-%s' % (layer, epoch, batch))\n",
    "            plt.clf()\n",
    "            \n",
    "            \n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.savefig('results/plots/loss'+ '%s-%s-%s' % (layer, epoch, batch))\n",
    "            plt.clf()\n",
    "            \n",
    "            data=np.concatenate([history.history['val_loss'],history.history['val_acc'],history.history['loss'],history.history['acc']])\n",
    "            np.savez_compressed('results/data/'+ '%s-%s-%s' % (layer, epoch, batch), data)\n",
    "    \n",
    "data= list(zip(score,data_dict.values()))\n",
    "with open('results/data.json','w') as outfile:\n",
    "    json.dump(data,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
