{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import helper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io as spio\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "rs = 2018\n",
    "random.seed(rs)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the matrix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24000, 24000, 24000, 24000, 24000, 24000, 24000, 24000, 24000,\n",
       "       24000], dtype=uint64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist = spio.loadmat(\"data/EMNIST/emnist-digits.mat\")\n",
    "x_train = emnist[\"dataset\"][0][0][0][0][0][0].astype(np.float32)\n",
    "y_train = emnist[\"dataset\"][0][0][0][0][0][1]\n",
    "\n",
    "x_test = emnist[\"dataset\"][0][0][1][0][0][0].astype(np.float32)\n",
    "y_test = emnist[\"dataset\"][0][0][1][0][0][1]\n",
    "\n",
    "x_train = x_train/x_train.max()\n",
    "x_test = x_test/x_train.max()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28, order = 'A').astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28, order = 'A').astype('float32')\n",
    "\n",
    "y_train = pd.get_dummies(y_train.ravel()).values\n",
    "y_test = pd.get_dummies(y_test.ravel()).values\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "y_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Tensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(rs)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST('data/', train=True, split='digits', download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST('data/', train=False, split='digits', transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # first convolutional layer\n",
    "        h_conv1 = self.conv1(x)\n",
    "        h_conv1 = F.relu(h_conv1)\n",
    "        h_conv1_pool = F.max_pool2d(h_conv1, 2)\n",
    "\n",
    "        # second convolutional layer\n",
    "        h_conv2 = self.conv2(h_conv1_pool)\n",
    "        h_conv2 = F.relu(h_conv2)\n",
    "        h_conv2_pool = F.max_pool2d(h_conv2, 2)\n",
    "\n",
    "        # fully-connected layer\n",
    "        h_fc1 = h_conv2_pool.view(-1, 320)\n",
    "        h_fc1 = self.fc1(h_fc1)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "        \n",
    "        # classifier output\n",
    "        output = self.fc2(h_fc1)\n",
    "        output = F.log_softmax(output)\n",
    "        return output, h_fc1, h_conv2, h_conv1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rhammond/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/240000 (0%)]\tLoss: 2.311247\n",
      "Train Epoch: 0 [10000/240000 (4%)]\tLoss: 2.300393\n",
      "Train Epoch: 0 [20000/240000 (8%)]\tLoss: 2.294405\n",
      "Train Epoch: 0 [30000/240000 (12%)]\tLoss: 2.284554\n",
      "Train Epoch: 0 [40000/240000 (17%)]\tLoss: 2.266620\n",
      "Train Epoch: 0 [50000/240000 (21%)]\tLoss: 2.250103\n",
      "Train Epoch: 0 [60000/240000 (25%)]\tLoss: 2.216611\n",
      "Train Epoch: 0 [70000/240000 (29%)]\tLoss: 2.173913\n",
      "Train Epoch: 0 [80000/240000 (33%)]\tLoss: 2.111950\n",
      "Train Epoch: 0 [90000/240000 (38%)]\tLoss: 1.996463\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "start = datetime.now() \n",
    "for epoch in range(10):\n",
    "    train(epoch)\n",
    "    print('Time elapsed {}'.format(datetime.now() - start))\n",
    "    test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all():\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        # run the data through the network\n",
    "        output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "        \n",
    "        # compare prediction to ground truth\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    perc_correct = 100. * correct / len(test_loader.dataset)\n",
    "    return perc_correct\n",
    "\n",
    "# Show the network's predicted class for an arbitrary set of `nshow` images from the MNIST test set\n",
    "def test_viz(nshow=10):\n",
    "    \n",
    "    # grab a random subset of the data\n",
    "    testiter = iter(test_loader)\n",
    "    images, target = testiter.next()\n",
    "    perm = np.random.permutation(images.size()[0])\n",
    "    sel = torch.LongTensor(perm[:nshow])\n",
    "    images = images[sel]\n",
    "    data = Variable(images, volatile=True)\n",
    "    \n",
    "    # get predictions from the network\n",
    "    output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    pred = pred.numpy().flatten()\n",
    "    \n",
    "    # plot predictions along with the images\n",
    "    for i in range(nshow):\n",
    "        ax = plt.subplot(1, nshow, i+1)\n",
    "        imshow(utils.make_grid(images[i]))\n",
    "        plt.title(str(pred[i]))\n",
    "\n",
    "# Display an image from the MNIST data set\n",
    "def imshow(img):\n",
    "    img = 1 - (img * 0.3081 + 0.1307) # invert image pre-processing\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "# checkpoint = torch.load('models/convnet_mnist_pretrained.pt')\n",
    "# model = Net()\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# model.eval()\n",
    "# print('Convnet has been loaded successfully.')\n",
    "# print('Running through the test set...')\n",
    "# test_acc = test_all()\n",
    "# print(' Accuracy on the test set is %.2f percent correct!' % test_acc)\n",
    "# print(\"\")\n",
    "# print(\"Here are some predictions from the network.\")\n",
    "# print(\"The images are shown below their predicted class labels.\")\n",
    "# test_viz()\n",
    "\n",
    "def get_random_subset(digit_select, npairs=20):\n",
    "    # digit_select: which digit do we want to get images for?\n",
    "    testiter = iter(test_loader)\n",
    "    images, target = testiter.next()\n",
    "    indices = np.flatnonzero(target.numpy() == digit_select)    \n",
    "    np.random.shuffle(indices)\n",
    "    indx1 = torch.LongTensor(indices[:npairs])\n",
    "    indx2 = torch.LongTensor(indices[npairs:npairs*2])\n",
    "    images1 = images[indx1]\n",
    "    images2 = images[indx2]\n",
    "    plt.figure(1,figsize=(4,40))\n",
    "    plot_image_pairs(images1,images2)\n",
    "    return images1, images2\n",
    "\n",
    "def plot_image_pairs(images1,images2,scores_net=[],scores_people=[]):\n",
    "    # images1 : list of images (each image is a tensor)\n",
    "    # images2 : list of images (each image is a tensor)\n",
    "    # scores_net (optional) : network similarity score for each pair\n",
    "    # scores_people (optional) : human similarity score for each pair\n",
    "    npairs = images1.size()[0]\n",
    "    assert images2.size()[0] == npairs\n",
    "    for i in range(npairs):\n",
    "        ax = plt.subplot(npairs, 1, i+1)\n",
    "        imshow(utils.make_grid([images1[i], images2[i]]))\n",
    "        mytitle = ''\n",
    "        if len(scores_net)>0:\n",
    "            mytitle += 'net %.2f, ' % scores_net[i] \n",
    "        if len(scores_people)>0:\n",
    "            mytitle += 'human. %.2f' % scores_people[i]\n",
    "        if mytitle:\n",
    "            plt.title(mytitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1, images2 = get_random_subset(8,npairs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
